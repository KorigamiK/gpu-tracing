<meta charset="utf-8">
<link rel="icon" type="image/png" href="../favicon.png">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

                                  **Ray Tracing: Moving to the GPU**
                                           [Arman Uguray][]
                                                <br>
                                                Draft
                                                <br>

Introduction
====================================================================================================
If you ever build your own path tracer you'll quickly appreciate the quality of the images you can
get with relatively few lines of code. You'll also notice that rendering an image takes a long time.
A path tracer estimates _Global Illumination_ by tracing tens of millions of rays through the scene.
The results are photorealistic because they can be close to physically accurate but that comes at a
high computational cost. This has historically made path tracing unsuitable for real-time
applications.

Fortunately -- like most applications in Computer Graphics -- the algorithm lends itself very well
to parallelism and we can use a GPU to exploit this to massively speed up render times. In this book
I'll show you a way to write a GPU accelerated path tracer targeting a modern GPU API. I'll focus on
building a renderer that can produce high quality and correct images using a fairly simple design.
It won't be full-featured and its performance will be limited, but it can serve as a great starting
point for extensions and experiments with more advanced GPU techniques.

This book primarily focuses on GPU programming fundamentals. It follows the same progression found
in Peter Shirley's [_Ray Tracing In One Weekend_][RTIOW], which is the first in a [brilliant series
of books](https://raytracing.github.io) that guide you through building a CPU path tracer. I cover a
lot of the same material but I highly recommend that you at least complete _In One Weekend_ before
embarking on building the GPU version -- doing so will teach you the path tracing algorithm in a
much simpler way and it will make you appreciate both the advantages and the challenges of moving to
a GPU-based architecture.

Targeting a GPU requires you to program in a way that is different from how you would typically
approach a CPU program. Taking full advantage of the hardware requires very careful tuning based on
its architecture and capabilities. I discuss performance considerations when they matter but I have
generally avoided optimizations and instead prioritized simplicity.

With that being said, the code you'll build by the end of this book can achieve high frame rates on
a decent enough GPU.[^syntax] The performance will highly depend on your GPU but you should
generally be able to achieve near-interactive rates with a small amount of tuning.

If you run into any problems with your implementation, have general questions or corrections, or
would like to share own ideas or work, check out [the GitHub Discussions forum][discussions].

Finally, I'd like to extend a special thank you to [Steve Hollasch][] and [Trevor David Black][] for
their guidance and for allowing me to host this book under the [RayTracing GitHub
project][rt-project].

[^syntax]: A BVH-accelerated implementation can render a modified version of the _Ray Tracing In One
Weekend_ cover scene with ~32,000 spheres, 16 ray bounces per pixel, and a resolution of 2048x1536
on a 2022 _Apple M1 Max_ in 15 milliseconds. The same renderer performs very poorly on a 2019 _Intel
UHD Graphics 630_ which takes more than 200ms.

GPU APIs
--------
Interfacing with a GPU and writing programs for it typically requires the use of a special API. This
interface depends on your operating system and GPU vendor. You often have various options depending
on the capabilities you want. For example, an application that wants to get the most juice out of a
NVIDIA GPU for general purpose computations may choose to target CUDA, while a developer who prefers
broad hardware compatibility for a graphical mobile game may choose OpenGL ES or Vulkan. Direct3D
(D3D) is the main graphics API on Microsoft platforms while Metal is the preferred framework on
systems from Apple. Vulkan, D3D12, and Metal even support an API specifically to accelerate ray
tracing.

You can implement this book using any API or framework that you prefer, though I generally assume
that you are working with a graphics API. In my examples I use an API based on [WebGPU][webgpu]
which I think maps well to all modern graphics APIs like Metal, Vulkan, and D3D12. The code
examples should be easy to adapt to those libraries. I avoided using ray tracing APIs (such as
[DXR][dxr] or [Vulkan Ray Tracing][vkrt]) to show you how to implement similar functionality on
your own.

<!-- TODO: Maybe this is better to list in a references section at the bottom -->
If you're looking to implement this in CUDA, you may also be interested in Roger Allen's
[blog post][rtiow-cuda] titled _Accelerated Ray Tracing in One Weekend in CUDA_.

Example Code
------------
Like _Ray Tracing In One Weekend_, you'll find code examples throughout the book. I use [Rust][] as
the driving language but you can choose any language that supports your GPU API of choice. I avoid
most esoteric aspects of Rust to keep the code easily understandable to a large audience. On the few
occasions where I had to resort to a potentially unfamiliar Rust-ism, I provide a C example to add
clarity.

I provide the finished source code for this book on [GitHub][gt-project] as a reference but I
encourage you to type in your own code. I decided to also provide a minimal source template that you
can use as a starting point if you want to follow along in Rust. The template provides a small
amount of setup code for the windowing logic to help get you started.

### A note on Rust, Libraries, and APIs

I chose Rust for this project because of it's ease of use and portability. It also happens to be the
language that I tend to be most productive in.

An important aspect of Rust is that a lot of common functionality is provided by libraries outside
its standard library. I tried to avoid external dependencies as much as possible except for the
following:

* I use *[wgpu][]* to interact with the GPU. This is a native graphics API based on
  WebGPU. It's portable and allows the example code to run on Vulkan, Metal, Direct3D 11/12, OpenGL
  ES 3.1, as well as WebGPU and WebGL via WebAssembly.

  wgpu also has [native bindings in other languages](https://github.com/gfx-rs/wgpu-native).

* I use [*winit*](https://docs.rs/winit/latest/winit/) which is a portable windowing library. I use
  it to view the rendered image in real-time and to make the example code interactive.

* For ease of Rust development I use [*anyhow*](https://docs.rs/anyhow/latest/anyhow/) and
  [*bytemuck*](https://docs.rs/bytemuck/latest/bytemuck/). *anyhow* is a popular error handling
  utility and integrates seamlessly. *bytemuck* provides a safe abstraction for the equivalent of
  `reinterpret_cast` in C++, which normally requires [`unsafe`][rust-unsafe] Rust. I use it to
  bridge CPU data types with their GPU equivalents.

* Lastly, I use [*pollster*](https://docs.rs/pollster/latest/pollster/) to execute asynchronous
  wgpu API functions (which is only called from a single line).

[wgpu][] is the most important dependency as it defines how the example code  interacts with the
GPU. Every GPU API is different but their abstractions for the general concepts used in this book
are fairly similar. I will highlight these differences occasionally where they matter.

A large portion of the example code runs on the GPU. Every graphics API defines a programming
language -- a so called **shading language** -- for authoring GPU programs. wgpu is based on WebGPU,
as such my GPU code examples are written in the *WebGPU Shading Language* (WGSL)[^syntax].
<!-- Have GLSL examples too? -->

The [wgpu API documentation](https://docs.rs/wgpu/latest/wgpu/) and the
[WebGPU](https://www.w3.org/TR/webgpu/) and [WGSL](https://www.w3.org/TR/WGSL/) specifications are
great references to keep handy when you're
developing.

With all of that out of the way, let's get started!

[^syntax]: wgpu also supports shaders in the
[SPIR-V](https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html) binary format. You could
in theory write your shaders in a shading language that can compile to SPIR-V (such as OpenGL's GLSL
and Direct3D's HLSL) as long as you avoid any language features that can't be expressed in WGSL.

Windowing and GPU Setup
====================================================================================================
The first thing to decide is how you want to view your image. One option is to write the output from
the GPU to a file. I think a more fun option is to display the image inside an application window.
I prefer this approach because it allows you to see your rendering as it resolves over time and it
will allow you to make your application interactive later on. The downside is that it requires a
little bit of wiring.

First, your program needs a way to interact with your operating system to create and manage a
window. Next, you need a way to coordinate your GPU workloads to output a sequence of images at the
right time for your OS to be able to composite it inside the window and send it to your display.

Every operating system with a graphical UI provides a native *windowing API* for this purpose.
Graphics APIs typically define some way to integrate with a windowing system. You'll have various
libraries to choose from depending on your OS and programming language. You mainly need to make sure
that the windowing API or UI toolkit you choose can integrate with your graphics API.

In my examples I use *winit* which is a Rust framework that integrates smoothly with wgpu. I put
together a [project template][gt-template] that sets up the library boilerplate for the window
handling. You're welcome to use it as a starting point.

The setup code isn't a lot, so I'll briefly go over the important pieces in this chapter.

The Event Loop
--------------
The first thing the template does is create a window and associate it with an *event loop*. The OS
sends a message to the application during important "events" that the application should act on,
such as a mouse click or when the window gets resized. Your application can wait for these events
and handle them as they arrive by looping indefinitely:


    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    use {
        anyhow::{Context, Result},
        winit::{
            event::{Event, WindowEvent},
            event_loop::{ControlFlow, EventLoop},
            window::WindowBuilder,
        },
    };

    const WIDTH: u32 = 800;
    const HEIGHT: u32 = 600;

    fn main() -> Result&lt()&gt {
        let event_loop = EventLoop::new();
        let window_size = winit::dpi::LogicalSize::new(WIDTH, HEIGHT);
        let window = WindowBuilder::new()
            .with_inner_size(window_size)
            .with_title("GPU Path Tracer".to_string())
            .build(&event_loop)?;

        // TODO: initialize renderer

        event_loop.run(move |event, _, control_flow| {
            *control_flow = ControlFlow::Poll;
            match event {
                Event::WindowEvent { event, .. } => match event {
                    WindowEvent::CloseRequested => *control_flow = ControlFlow::Exit,
                    _ => (),
                }
                Event::RedrawRequested(_) => {
                    // TODO: draw frame
                }
                Event::MainEventsCleared => {
                    // draw repeatedly
                    window.request_redraw();
                }
                _ => ()
            }
        });
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [main-initial]: <kbd>[main.rs]</kbd> Creating a window and handling window events]

This code creates a window titled "GPU Path Tracer" and kicks off an event loop.
`event_loop.run()` internally waits for window events and notifies your application by calling the
lambda function that it gets passed as an argument.

The lambda function only handles a few events for now. The most important one is `RedrawRequested`
which is the signal to render and present a new frame. `MainEventsCleared` is simply an event that
gets sent when all pending events have been processed. We call `window.request_redraw()` to draw
repeatedly -- this triggers a new `RedrawRequested` event which is followed by another
`MainEventsCleared`, which requests a redraw,  and so on until someone closes the window.

Running this code should bring up an empty window like this:

  ![Figure [empty-window]: Empty Window](../images/img-1-empty-window.png)

GPU and Surface Initialization
------------------------------
The next thing the template does is establish a connection to the GPU and configure a surface. The
surface manages a set of *textures* that allow the GPU to render inside the window.

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    async fn connect_to_gpu(window: &Window) -> Result&lt(wgpu::Device, wgpu::Queue, wgpu::Surface&gt {
        use wgpu::TextureFormat::{Bgra8Unorm, Rgba8Unorm};
        // Create an "instance" of wgpu. This is the entry-point to the API.
        let instance = wgpu::Instance::default();

        // Create a drawable "surface" that is associated with the window.
        let surface = unsafe { instance.create_surface(&window) }?;

        // Request a GPU that is compatible with the surface. If the system has multiple GPUs then
        // pick the high performance one.
        let adapter = instance
            .request_adapter(&wgpu::RequestAdapterOptions {
                power_preference: wgpu::PowerPreference::HighPerformance,
                force_fallback_adapter: false,
                compatible_surface: Some(&surface),
            })
            .await
            .context("failed to find a compatible adapter")?;

        // Connect to the GPU. "device" represents the connection to the GPU and allows us to create
        // resources like buffers, textures, and pipelines. "queue" represents the command queue that
        // we use to submit commands to the GPU.
        let (device, queue) = adapter
            .request_device(&wgpu::DeviceDescriptor::default(), None)
            .await
            .context("failed to connect to the GPU")?;

        // Configure the texture memory backs the surface. Our renderer will draw to a surface texture
        // every frame.
        let caps = surface.get_capabilities(&adapter);
        let format = caps
            .formats
            .into_iter()
            .find(|it| matches!(it, Rgba8Unorm | Bgra8Unorm))
            .context("could not find preferred texture format (Rgba8Unorm or Bgra8Unorm)")?;
        let size = window.inner_size();
        let config = wgpu::SurfaceConfiguration {
            usage: wgpu::TextureUsages::RENDER_ATTACHMENT,
            format,
            width: size.width,
            height: size.height,
            present_mode: wgpu::PresentMode::AutoVsync,
            alpha_mode: caps.alpha_modes[0],
            view_formats: vec![],
        };
        surface.configure(&device, &config);

        Ok((device, queue, surface))
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [main-initial]: <kbd>[main.rs]</kbd> The connect_to_gpu function]

The code that sets this all up is a bit wordy. I'll quickly go over the important bits:

1. What the first ~20 lines do is request a connection to a GPU that is compatible with the
   window. The bit about `wgpu::PowerPreference::HighPerformance` is a hint to the API that we want
   the higher-powered GPU if the current system has more than one available.

2. The rest of the function configures the dimensions, pixel format, and presentation mode of the
   surface. `Rgba8Unorm` and `Bgra8Unorm` are common pixel formats that store each color component
   (red, green, blue, and alpha) as an 8-bit unsigned integer. The "unorm" part stands for "unsigned
   normalized", which means that our rendering code can represent the component values as a real
   number in the range `[0.0, 1.0]`. We set the size to simply span the entire window.

   The bit about `wgpu::PresentMode::AutoVsync` tells the surface to synchronize the presentation of
   each frame with the display's refresh rate. The surface will manage an internal queue of textures
   for us and we will render to them as they become available. This prevents a visual artifact known
   as "tearing" (which can happen when frames get presented faster than the display refresh rate) by
   setting up the renderer to be *v-sync locked*. We will discuss some of the implications of this
   later on.

   The last bit that I'll highlight here is `wgpu::TextureUsage::RENDER_ATTACHMENT`. This just
   indicates that we are going to use the GPU's rendering function to draw directly into the surface
   textures.

After setting all this up the function returns 3 objects: A `wgpu::Device` that represents the
connection to the GPU, a `wgpu::Queue` which we'll use to issue commands to the GPU, and a
`wgpu::Surface` that we'll use to present frames to the window. We will talk a lot about the first
two when we start putting together our renderer in the next chapter.

You may have noticed that the function declaration begins with `async`. This marks the function as
*asynchronous* which means that it doesn't return its result immediately. This is only necessary
because the API functions that we invoke (`wgpu::Instance::request_adapter` and
`wgpu::Adapter::request_device`) are asynchronous functions. The `.await` keyword is syntactic sugar
that makes the asynchronous calls appear like regular (synchronous) function calls. What happens
under the hood is somewhat complex but I wouldn't worry about this too much since this is the one
and only bit of asynchronous code that we will encounter. If you want to learn more about it, I
recommend checking out the [Rust Async Book](https://rust-lang.github.io/async-book/).

### Completing Setup

Finally, the `main` function needs a couple updates: first we make it `async` so that it we can
"await" on `connect_to_gpu`. Technically the `main` function of a program cannot be async and
running an async function requires some additional utilities. There are various alternatives but I
chose to use a library called `pollster`. The library provides a special macro (called `main`) that
takes care of everything. Again, this is the only asynchronous code that we'll encounter so don't
worry about what it does.

The second change to the main function is where it handles the `RedrawRequested` event. For every
new frame, we first request the next available texture from the surface that we just created. The
queue has a limited number of textures available. If the CPU outpaces the GPU (i.e. the GPU takes
longer than a display refresh cycle to finish its tasks), then calling
`surface.get_current_texture()` can block until a texture becomes available.

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust highlight
    #[pollster::main]
    async fn main() -> Result&lt()&gt {
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
        let event_loop = EventLoop::new();
        let window_size = winit::dpi::LogicalSize::new(WIDTH, HEIGHT);
        let window = WindowBuilder::new()
            .with_inner_size(window_size)
            .with_title("GPU Path Tracer".to_string())
            .build(&event_loop)?;

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust highlight
        let (device, queue, surface) = connect_to_gpu(&window).await?;


    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
        // TODO: initialize renderer

        event_loop.run(move |event, _, control_flow| {
            *control_flow = ControlFlow::Poll;
            match event {
                Event::WindowEvent { event, .. } => match event {
                    WindowEvent::CloseRequested => *control_flow = ControlFlow::Exit,
                    _ => (),
                }
                Event::RedrawRequested(_) => {
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust highlight
                    // Wait for the next available frame buffer.
                    let frame: wgpu::SurfaceTexture = surface
                        .get_current_texture()
                        .expect("failed to get current texture");


    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
                    // TODO: draw frame
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust highlight

                    frame.present();
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
                }
                Event::MainEventsCleared => {
                    // draw repeatedly
                    window.request_redraw();
                }
                _ => ()
            }
        });
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [main-setup-complete]: <kbd>[main.rs]</kbd> Putting together the initial main function]

Once a frame texture becomes available, the example issues a request to display it as soon as
possible by calling `frame.present()`. All of our rendering work will be scheduled before this call.

That was a lot of boilerplate -- this is sometimes necessary to interact with OS resources. With all
of this in place, we can start building a real-time renderer.

### A note on error handling in Rust
If you're new to Rust, some of the patterns above may look unfamiliar. One of these is error
handling using the `Result` type. I use this pattern frequently enough that it's worth a quick
explainer.

A `Result` is a variant type that can hold either a success (`Ok`) value or an error (`Err`) value.
The types of the `Ok` and `Err` variants are generic:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    pub enum Result&ltT, E&gt {
        Ok(T),
        Err(E),
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Figure [rust-result]: The `Result` type]

`T` and `E` can be any type. It's common for a library to define its own error types to represent
various error conditions.

The idea is that a function returns a `Result` if it has a failure mode. A caller must check the
status of the `Result` to unpack the return value or recover from an error.

In a C program, a common way to handle an error is to return early from the calling function and
and perhaps return an entirely new error. For example:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C
    bool function_with_a_result(Foo* out_result);

    int main() {
        Foo foo;
        if (!function_with_result(&foo)) {
            return -1;
        }

        // ...do something with `foo`...

        return 0;
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Rust provides the `?` operator to automatically unpack a `Result` and return early if it holds an
error. A Rust version of the C program above could be written like this:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    fn function_with_result() -> Result&ltFoo, FooError&gt {...}

    fn caller() -> Result&lt(), FooError&gt {
        let foo: Foo = function_with_result()?;

        // ...do something with `foo`...

        Ok(())
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If `function_with_result()` returns an error, the `?` operator will cause `caller` to return and
propagate the error value. This works as long as `caller` and `function_with_result` either return
the same error type or types with a known conversion. There are various other ways to handle an
error:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    fn function_with_result() -> Result&ltFoo, FooError&gt {...}

    fn caller() -> Result&lt(), BarError&gt {
        ...
        if let Err(e) = function_with_result() {
            println!("got a foo error: {:?}", e);
            return Err(BarError::from_foo(e));
        }
        ...
        let foo = function_with_result().map_err(BarError::from_foo)?;
        ...
        let Ok(foo) = function_with_result() else {
            panic!("Didn't work the second time");
        }
        ...
        let foo = match function_with_result() {
            Ok(foo) => foo,
            Err(e) => panic!("failed again");
        };
        ...
        Ok(())
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I like to keep things simple in my code examples and use the `?` operator. Instead of defining
custom error types and conversions, I use a catch all `Error` type from a library called *anyhow*.
You'll often see the examples include `anyhow::Result` (an alias for `Result&ltT, anyhow::Error&gt`)
and `anyhow::Context`. The latter is useful for adding an error message while converting to an
`anyhow::Error`:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Rust
    fn caller() -> anyhow::Result&lt()&gt {
        let foo: Foo = function_with_result().context("failed to get foo")?;

        // ...do something with `foo`...

        Ok(())
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can read more about the `Result` type in [its module
documentation](https://doc.rust-lang.org/std/result/index.html).


Drawing Pixels
====================================================================================================
At this stage, we have code that brings up a window, connects to the GPU, and sets up a queue of
textures (also known as a *swap chain*) that is synchronized with the display. In Computer
Graphics, the term "texture" is generally used in the context of *texture mapping*, which is a
technique to apply detail to geometry using data stored in memory. A very common application is to
map (in the mathematical sense) color data from the pixels of a 2D image onto the surface of a 3D
polygon.

Texture mapping is so essential to real-time graphics that all modern GPUs are equipped with
specialized hardware to speed up texture operations. It's not uncommon for a modern video game to
use texture assets (for meterials, colors, surface normals, etc) that take up hundreds of megabytes.
Memory access is often the performance bottleneck for a GPU, so GPUs provide features like dedicated
texture memory caches, sampling hardware, and compression -- all geared towards increasing texture
data throughput.

The hardware texture features are useful for any type of data that follows similar memory access
patterns and they don't have to be used strictly for texture mapping. The surface textures in the
code are such an example because we are going to use them to store and display the output of the
renderer[^syntax].

In wgpu (as well as all the other APIs that it is compatible with) a *texture object* represents
texture memory that can be used in three main ways: texture mapping, shader storage, and the target
of a *render pass*[^^syntax]. A surface texture is special because wgpu allows a surface texture to
be used only as a render target.

Not all native APIs have this restriction. For instance, both Metal and Vulkan allow their version
of a surface texture -- a *frame buffer* (Metal) or *swap chain* (Vulkan) texture -- to be
configured for other usages. This sometimes comes with a warning about impaired performance and is
not guaranteed to be supported by the hardware.

wgpu doesn't provide any other option so I'm going to start by implementing a render pass. This is
an essential and very widely used function of the GPU, so it's worth learning about.


[^syntax]: *Technically, we are mapping a 2D grid of pixels that is stored in memory to another
region of memory that shows up on the screen, so one could say this is still texture mapping.

[^^syntax]: See [`wgpu::TextureUsages`](https://docs.rs/wgpu/0.17.0/wgpu/struct.TextureUsages.html).

[Arman Uguray]:       https://github.com/armansito
[Steve Hollasch]:     https://github.com/hollasch
[Trevor David Black]: https://github.com/trevordblack
[RTIOW]:              https://raytracing.github.io/books/RayTracingInOneWeekend.html
[rt-project]:         https://github.com/RayTracing/
[gt-project]:         https://github.com/RayTracing/gpu-tracing/
[gt-template]:        https://github.com/RayTracing/gpu-tracing/blob/dev/code/template
[discussions]:        https://github.com/RayTracing/gpu-tracing/discussions/
[dxr]:                https://en.wikipedia.org/wiki/DirectX_Raytracing
[vkrt]:               https://www.khronos.org/blog/ray-tracing-in-vulkan
[rtiow-cuda]:         https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/
[webgpu]:             https://www.w3.org/TR/webgpu/
[Rust]:               https://www.rust-lang.org/
[rust-unsafe]:        https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html
[wgpu]:               https://wgpu.rs

<!-- Markdeep: https://casual-effects.com/markdeep/ -->
<link rel='stylesheet' href='../style/book.css'>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script src="markdeep.min.js"></script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
